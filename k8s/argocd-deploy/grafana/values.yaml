# kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
# grafana-cli --homepath "/usr/share/grafana" admin reset-admin-password <new password>
traefik-ingress:
  dnsNames:
    - dns: grafana.rannet.duckdns.org
      services:
        - kind: Service
          name: grafana
          port: service

vault-auth:
  vaultConnectionName: vault-secrets-operator/default
  authMethods:
    - name: on-prem
      # match this up with secret
      vaultAuthName: grafana-vso-connection-auth
      method: kubernetes
      mount: in-cluster
      config:
        role: grafana
        serviceAccount: grafana
        audiences:
          - vault

grafana:
  grafana.ini:
    server:
      domain: grafana.rannet.duckdns.org
      # forcing https here, for some reason it defaults %(protocol)s goes to http
      root_url: "https://%(domain)s"
    auth.generic_oauth:
      name: RanNet OIDC
      enabled: true
      allow_sign_up: true
      scopes: openid profile email
      empty_scopes: false
      auth_url: $__file{/etc/secrets/oidc_authentik_secrets/authorize_url}
      token_url: $__file{/etc/secrets/oidc_authentik_secrets/token_url}
      api_url: $__file{/etc/secrets/oidc_authentik_secrets/userinfo_url}
      signout_url: $__file{/etc/secrets/oidc_authentik_secrets/signout_url}
      client_id: $__file{/etc/secrets/oidc_authentik_secrets/client_id}
      client_secret: "$__file{/etc/secrets/oidc_authentik_secrets/client_secret}"
      role_attribute_strict: true
      role_attribute_path: "contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'Grafana Editors') && 'Editor' || 'Viewer'"

  extraSecretMounts:
    - name: oidc-authentik-secret-mount
      secretName: oidc-authentik-secrets
      defaultMode: 0440
      mountPath: /etc/secrets/oidc_authentik_secrets
      readOnly: true

  podDisruptionBudget:
    minAvailable: 1

  # this just started to fail one day, no idea
  initChownData:
    enabled: false

  persistence:
    enabled: true
    existingClaim: grafana # I think this is from the ~8 version of the chart

    # below is now to populate the the pvc in templates (to carry over the old
    # PV w/ the old "grafana" pvc)
    storageClassName: longhorn
    size: 1Gi
    volumeName: pvc-82b7ece5-d11a-44c4-8e21-f9b4fca70a42

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Prometheus
        type: prometheus
        url: http://prometheus-server

  # https://github.com/dotdc/grafana-dashboards-kubernetes
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'grafana-dashboards-kubernetes'
        orgId: 1
        folder: 'Kubernetes'
        type: file
        disableDeletion: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/grafana-dashboards-kubernetes
  dashboards:
    grafana-dashboards-kubernetes:
      k8s-system-api-server:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json
        token: ''
      k8s-system-coredns:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json
        token: ''
      k8s-views-global:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json
        token: ''
      k8s-views-namespaces:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json
        token: ''
      k8s-views-nodes:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json
        token: ''
      k8s-views-pods:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json
        token: ''
